---
title: "Multi-Objective Optimization"
classes: wide
toc: true
toc_sticky: true
categories: 
  - Post
last_modified_at: 2019-06-30
tags:
  - multi-task learning
  - reinforcement learning
  - machine learning
  - AutoML
---


**Summary**: Multi-Objective Optimization to Solve Hyperparameters of Regularization in Machine Learning. This idea could be further implemented to multi-task learning, or deep learning/reinforcement learning that usually require tedious hyperparameter tuning. This result comes from my research on [SNAS](https://github.com/haydenz/SNAS-Series).

## Problem Background: 
- The objective function we want to optimize is composed of two parts -- one is the standard cross-entropy loss; the other is a regularization term to reduce overfitting. 
  - The regularization term is the sum of latency costs (parameter size, memory access cost, ect) for the purpose of reducing training time. 
  - Each kind of latency cost is multiplied by a hyperparameter.

- Usually, we treat these hyperparameters as Lagrange multiplier and tune them manually, getting constrained optimal cross-entropy loss. 
- However, we can also view the constraints as objects in other dimensions than that of the cross-entropy loss. Then the objective function is transformed from 1-dimension to multi-dimension. 
- With this transformation, we need to find a proper way to calculate the gradient of the multi-objective in each batch. 
- In the end, tedious manual tuning hyperparameters is avoided.

## Solution: 
- First, to deal with multi-dimensions, we need to borrow the Pareto critical points [1] concept from economics.  
- Second,  we can solve the Pareto critical points by convex quadratic programming (qp) below,  which constructed by the Jacobian of the objective function, A. [1]
  <p align="center">
    <img src="https://haydenz.github.io/assets/media/snas/qp.png" alt="qp" title="Multi-Obj QP" width="" height="" />
  </p>
- Third, in each batch, we replace the backward codes of PyTorch with the result solved from duality qp. (I tried both self-written duality and quadprog to solve qp)


## Geometric Intuition:
The qp [1] might seems not very straightforward. So I proved that the intuition in [2] is equivalent to the qp. Such intuition is demonstrated below. 
  1. WLOG, we assume two variants x1, x2, two object functions f1, f2, and we want to minimize the vector (f1, f2).
  2.1 For a single objective, we can category space by two half-space (H-, H+) in **Figure 2**, where H- is the half-space when the opposite of gradient points to it, the objective will decrease.

     For 2D objectives, we can categorize the space into 4 kinds in **Figure 3**. The half-space {-, -} is the direction we want, to decrease both objects f1 and f2.

      <p align="center">
        <img src="https://haydenz.github.io/assets/media/snas/fig2.png" alt="half-space" title="Half Space" width="" height="" />
      </p>
      <p align="center">
        <img src="https://haydenz.github.io/assets/media/snas/fig3.png" alt="4space" title="Four Spaces" width="" height="" />
      </p>

  3. If we plot the gradients of all objectives, we get the triangle in **Figure 6**:  as we are approaching the Pareto critical points, the angle <-g1, -g2> would become wider.

     To guarantee our updated gradient always pointing to the half-space {-,-}, we choose a direction d perpendicular to -g1 + g2 = v in **Figure 8**. In a higher dimension, this direction is in line with the height of the Pyramid. The height turns out to be the vector calculated from the qp.
      <p align="center">
        <img src="https://haydenz.github.io/assets/media/snas/fig6.png" alt="cone" title="Gradient Cone" width="" height="" />
      </p>
      <p align="center">
        <img src="https://haydenz.github.io/assets/media/snas/fig8.png" alt="diversity" title="Diversity Plane" width="" height="" />
      </p>


## References:
  [1] [Steepest Descent Methods for Multicriteria Optimization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.457.9314&rep=rep1&type=pdf)

  [2] [Directed Multi-Objective Optimization](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.2942&rep=rep1&type=pdf)